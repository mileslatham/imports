'''
Miles Latham (05-05-20)
Quick tutorial on importing data into pandas. See associated readme file for more information.
'''
# import statements:
import pandas as pd  # the "as pd" component just allows you to reference pandas functions with the shortcut "pd."
import os  # used to specify filepaths

'''
This first part setting the filepath variable is just using os.path.join to specify where our file is located, which
in this case is in a subdirectory called 'data' (combined.csv is the name of our data file). This function just takes the 
subcomponents you specify (in this case 'data' and 'combined.csv', and joins them into a relative filepath. This is 
helpful because it remains consistent across different OSs that specify filepaths in different ways.
'''

data_filepath = os.path.join('data', 'combined.csv')

'''
Next, we use the pd.read_csv function to load the data. In this case, our data is a csv of confirmed cases and deaths by
county and metro area, by date. Take a look at the csv file to get a sense of how this is structured- it's a pretty clean
and easy-to-load file (this is generated by the great script Aaron wrote to automatically download our case data). 
For each county, there is one observation per day from late January to mid-April. 

The first argument to the read_csv() function is the file, which we've specified the location of above. 

dtype is another optional argument, which we're passing with a dictionary of two column ID's that we want to specify as 
strings (charvars). They are numeric ID variables, and if we let Pandas infer what type they are, it might decide 
they're numbers and remove leading zeroes which would make them hard to match on. 

There are many other optional arguments, but because this is a very cleanly-formatted CSV, that's all we need now. It's 
often necessary to specify the index column or the row containing the column names, but because column names are 
located in row 0 of this spreadsheet, pandas is able to infer that.
'''

covid = pd.read_csv(data_filepath, dtype={"CBSA Code": str, 'countyFIPS': str})

'''
We've now loaded our data into a dataframe called 'covid'. Next, I've included a few print statements to show how you 
can check a few of the dataframe's attributes. 

covid.shape shows you the dimensions of the dataframe, in (rows, columns) format. You can see we have 264096 obs.
covid.size shows you the total number of items in the dataframe (rows * columns).
covid.head(n=10) gives you the fist 10 rows of the dataframe. 
covid.columns gives you an index of the columns in the dataframe. 
'''

print(covid.shape)
print(covid.size)
print(covid.head(n=10))
print(covid.columns)

'''
You may notice that our first 10 observations are exactly the same in every category except for date, because we have a 
lot of date observations with zero casualties or deaths. This is because our data goes back to january, while the 
vast majority of counties in the US had zero cases until late february. Next, we'll quickly remove some of those 
empty observations to decrease the size of our dataset and make it a little faster for our program/graphs to load. 

The first line below sets our covid df to only include observations where the number of confirmed cases is greater
than or equal to one. 

The second fixes a minor issue- you'll notice that our countyFIPS values include decimals, which we remove by applying a 
lambda funtion which splits the countyFIPS values on the character '.' and then takes only the first element before
the split (specified by the [0]). 
'''

covid = covid[(covid['confirmed_case'] >= 1)]
covid['countyFIPS'] = covid['countyFIPS'].apply(lambda x: x.split('.')[0])
covid['Date'] = pd.to_datetime(covid['Date'])

'''
With the below print statements, you can see that the dataframe is now 58161 observations with confirmed_case >=1, and 
our countyFIPS variable is now just the countyFIPS code with no decimal or trailing 0's.
'''

print(covid.shape)
print(covid.head(n=5))


